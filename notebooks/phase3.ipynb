{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# %pip install -U hazm\n",
    "from hazm import *\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "def read_file():\n",
    "    documents_title = []\n",
    "    documents_content = []\n",
    "    documents_url = []\n",
    "    with open(os.path.join(os.getcwd(), 'Phase_1', 'assets', 'IR_data_news_12k.json'), encoding='UTF-8') as f:\n",
    "        data = json.load(f)\n",
    "        for i in data:\n",
    "            documents_title.append(data[i][\"title\"])\n",
    "            documents_content.append(data[i][\"content\"])\n",
    "            documents_url.append(data[i]['url'])\n",
    "    return documents_url, documents_title, documents_content\n",
    "\n",
    "\n",
    "docs_url, docs_title, docs_content = read_file()\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "from os import path\n",
    "import codecs\n",
    "from hazm.Normalizer import Normalizer\n",
    "# from Phase_1.src.utils import data_path\n",
    "default_stop_words = path.join(os.getcwd(), 'Phase_1', 'src', 'data', 'stopwords.dat')\n",
    "\n",
    "\n",
    "class StopWord:\n",
    "    \"\"\" Class for remove stop words\n",
    "\n",
    "         >>> StopWord().clean([\"در\",\"تهران\",\"کی\",\"بودی؟\"])\n",
    "         ['بودی؟', 'تهران', 'کی']\n",
    "         >>> StopWord(normal=True).clean([\"در\",\"تهران\",\"کی\",\"بودی؟\"])\n",
    "         ['بودی؟', 'تهران']\n",
    "\n",
    "         \"\"\"\n",
    "\n",
    "    def __init__(self, file_path=default_stop_words, normal=False):\n",
    "        self.file_path = file_path\n",
    "        self.normal = normal\n",
    "        self.normalizer = Normalizer().normalize\n",
    "        self.stop_words = self.init(file_path, normal)\n",
    "\n",
    "    def init(self, file_path, normal):\n",
    "        if not normal:\n",
    "            return set(\n",
    "                line.strip(\"\\r\\n\") for line in codecs.open(file_path, \"r\", encoding=\"utf-8\").readlines())\n",
    "        else:\n",
    "            return set(\n",
    "                self.normalizer(line.strip(\"\\r\\n\")) for line in\n",
    "                codecs.open(file_path, \"r\", encoding=\"utf-8\").readlines())\n",
    "\n",
    "    def set_normalizer(self, func):\n",
    "        self.normalizer = func\n",
    "        self.stop_words = self.init(self.file_path, self.normal)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return item in self.stop_words\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.stop_words)\n",
    "\n",
    "    def clean(self, iterable_of_strings, return_generator=False):\n",
    "        if return_generator:\n",
    "            return filter(lambda item: not self[item], iterable_of_strings)\n",
    "        else:\n",
    "            return list(filter(lambda item: not self[item], iterable_of_strings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_content(content):\n",
    "    str_empty = ' '\n",
    "    stemmer = Stemmer()\n",
    "    content = normalizer.normalize(content)\n",
    "    content = word_tokenize(content)\n",
    "    content = StopWord(normal=False).clean(content)\n",
    "    content = [stemmer.stem(word) for word in content]\n",
    "    content = str_empty.join(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(docs_content)):\n",
    "    docs_content[i] = preprocess_content(docs_content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Connect to 'http://localhost:9200'\n",
    "es = Elasticsearch(\"http://localhost:55001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "گزار خبرگزار فارس کنفدراسیون فوتبال آسیا AFC نامه رسم فدراسیون فوتبال ایر باشگاه گیت پسند قرعه کش جا باشگاه فوتسال آسیا اساس ۲۵ فروردین ماه ۱۴۰۱ مراس قرعه کش جا باشگاه فوتسال آسیا مالز برگزار باشگاه گیت پسند بعنو قهر فوتسال ایر سال ۱۴۰۰ مسابق کرده_اس گیت پسند تجربه ۳ دوره حضور جا باشگاه فوتسال آسیا سه دوره فینال مسابق قهرمان مقا دوم بدس آورده_اس انت پیام/\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def generate_data():\n",
    "    data = []\n",
    "    for doc_content, doc_url, doc_title in zip(docs_content, docs_url, docs_title):\n",
    "        if doc_content and doc_url and doc_title:        \n",
    "            data.append({\n",
    "                \"_index\": \"ir-news-index\",\n",
    "                \"_id\" : uuid.uuid4(),\n",
    "                \"_source\": {\n",
    "                    \"content\": doc_content,\n",
    "                    \"url\": doc_url,\n",
    "                    \"title\": doc_title\n",
    "                }\n",
    "            })\n",
    "    return data\n",
    "\n",
    "data=generate_data()\n",
    "print(docs_content[0])\n",
    "response = helpers.bulk(es, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c2cd5a1955132356c9d3b85a8a352ec1c2b340d270ef36678734e3eb3412b7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}